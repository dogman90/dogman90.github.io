<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      (머신러닝) 12. 역전파 (back propagation) &middot; 공부 기록 블로그
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/main.css">
  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-precomposed.png">
  <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  
  <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_SVG"> </script>
  <script type="text/x-mathjax-config">
MathJax.Hub.Config({ tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], processEscapes: true } });
  </script>
  
</head>


  <body>

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <div class="sidebar-personal-info">
      <div class="sidebar-personal-info-section">
        <!-- <a href="http://gravatar.com/"> -->
          <a href="http://gravatar.com/"></a>
          <!-- <img src="http://www.gravatar.com/avatar/?s=350" title="View on Gravatar" alt="View on Gravatar" /> -->
          <!-- <img src="https://s.gravatar.com/avatar/fedd0cb9bf32323502b502a383136608?s=80" title="View on Gravatar" alt="View on Gravatar" style="margin:auto;" /> -->
          <img src="https://s.gravatar.com/avatar/fedd0cb9bf32323502b502a383136608?s=80" title="View on Gravatar" alt="View on Gravatar" style="margin:auto;" />
          

          
        </a>
      </div>
      <div class="sidebar-personal-info-section" >
        <p>개발자가 되고픈 사람. <br> 제대로 노력해서 1보를 이루자</p>
      </div>
      
    </div>
  </div>

  <nav class="sidebar-nav">
    
      
      
      

      

      <span class="">
        <a class="sidebar-nav-item " href="/">
          Home
        </a>

        
      </span>

    
      
      
      

      

      <span class="foldable">
        <a class="sidebar-nav-item " href="/blog/">
          Blog
        </a>

        
          
            
            
            
              <a class="sidebar-nav-item sidebar-nav-item-sub " href="/blog/categories/">
                Categories
              </a>
          
        
          
            
            
            
              <a class="sidebar-nav-item sidebar-nav-item-sub " href="/blog/tags/">
                Tags
              </a>
          
        
      </span>

    
      
      
      

      

      <span class="">
        <a class="sidebar-nav-item " href="/about/">
          About
        </a>

        
      </span>

    

    <span class="sidebar-nav-item">Currently v1.0.0</span>
  </nav>

  <div class="sidebar-item">
    <p>
    &copy; 2020 nobbaggu. This work is liscensed under <a href="http://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0</a>.
    </p>
  </div>

  <div class="sidebar-item">
    <p>
    Powered by <a href="http://jekyllrb.com">jekyll</a>
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <!-- <a href="/" title="Home" title="공부 기록 블로그">
              <img class="masthead-logo" src=""/>
            </a> -->
            <small>개발자가 되고 싶은 사람</small>
          </h3>
        </div>
      </div>

      <div class="container content">
        <div class="post">
  <h1 class="post-title">(머신러닝) 12. 역전파 (back propagation)</h1>
  <span class="post-date">10 Aug 2018</span>
   | 
  
    <a href="/blog/tags/#activation-function" class="post-tag">activation function</a>
  
    <a href="/blog/tags/#back" class="post-tag">back</a>
  
    <a href="/blog/tags/#forward" class="post-tag">forward</a>
  
    <a href="/blog/tags/#gradient-descent" class="post-tag">gradient descent</a>
  
    <a href="/blog/tags/#hidden-layer" class="post-tag">hidden layer</a>
  
    <a href="/blog/tags/#input-layer" class="post-tag">input layer</a>
  
    <a href="/blog/tags/#machine-learning" class="post-tag">machine learning</a>
  
    <a href="/blog/tags/#mlp" class="post-tag">MLP</a>
  
    <a href="/blog/tags/#multi-layer-perceptron" class="post-tag">multi layer perceptron</a>
  
    <a href="/blog/tags/#neural-network" class="post-tag">neural network</a>
  
    <a href="/blog/tags/#output-layer" class="post-tag">output layer</a>
  
    <a href="/blog/tags/#perceptron" class="post-tag">perceptron</a>
  
    <a href="/blog/tags/#propagation" class="post-tag">propagation</a>
  
    <a href="/blog/tags/#sigmoid" class="post-tag">sigmoid</a>
  
    <a href="/blog/tags/#머신러닝" class="post-tag">머신러닝</a>
  
    <a href="/blog/tags/#순전파" class="post-tag">순전파</a>
  
    <a href="/blog/tags/#시그모이드" class="post-tag">시그모이드</a>
  
    <a href="/blog/tags/#신경망" class="post-tag">신경망</a>
  
    <a href="/blog/tags/#에러" class="post-tag">에러</a>
  
    <a href="/blog/tags/#역전파" class="post-tag">역전파</a>
  
    <a href="/blog/tags/#은닉층" class="post-tag">은닉층</a>
  
    <a href="/blog/tags/#인공신경망" class="post-tag">인공신경망</a>
  
    <a href="/blog/tags/#퍼셉트론" class="post-tag">퍼셉트론</a>
  
    <a href="/blog/tags/#활성함수" class="post-tag">활성함수</a>
  
  
  <article>
    <h1 id="1-역전파-알고리즘">1. 역전파 알고리즘</h1>

<hr />

<p>신경망의 개념이 나온지는 꽤 되었지만 신경망을 학습시키는 방법을 몰라 오랫동안 사장된 이론이었다. 하지만 1970년대 미국의 한 대학원생은 신경망을 학습시킬 방법을 발견했고, 이것이 지금부터 설명할 역전파 이다. 사실 역전파 알고리즘은 우리가 regression 문제에서 이미 보았던 <span style="color: #000000;"><strong>gradient-descent(경사하강법)과 </strong>본질적으로 같다. </span>아무튼 역전파를 아래와 같은 3-2-2 층을 가진 신경망을 예시로 들어 설명하겠다.</p>

<p><img class="aligncenter size-medium wp-image-435" src="/images/2018/08/no-name-44-300x285.png" alt="" width="300" height="285" srcset="/images/2018/08/no-name-44-300x285.png 300w, /images/2018/08/no-name-44.png 570w" sizes="(max-width: 300px) 100vw, 300px" /></p>

<p><img src="https://latex.codecogs.com/gif.latex?\\&space;{&space;z&space;}_{&space;1&space;}^{&space;(2)&space;}={&space;\Theta&space;}_{&space;1,0&space;}^{&space;(1)&space;}{&space;a&space;}_{&space;0&space;}^{&space;(1)&space;}+{&space;\Theta&space;}_{&space;1,1&space;}^{&space;(1)&space;}{&space;a&space;}_{&space;1&space;}^{&space;(1)&space;}+{&space;\Theta&space;}_{&space;1,2&space;}^{&space;(1)&space;}{&space;a&space;}_{&space;2&space;}^{&space;(1)&space;}+{&space;\Theta&space;}_{&space;1,3&space;}^{&space;(1)&space;}{&space;a&space;}_{&space;3&space;}^{&space;(1)&space;}\\&space;{&space;z&space;}_{&space;2&space;}^{&space;(2)&space;}={&space;\Theta&space;}_{&space;2,0&space;}^{&space;(1)&space;}{&space;a&space;}_{&space;0&space;}^{&space;(1)&space;}+{&space;\Theta&space;}_{&space;2,1&space;}^{&space;(1)&space;}{&space;a&space;}_{&space;1&space;}^{&space;(1)&space;}+{&space;\Theta&space;}_{&space;2,2&space;}^{&space;(1)&space;}{&space;a&space;}_{&space;2&space;}^{&space;(1)&space;}+{&space;\Theta&space;}_{&space;2,3&space;}^{&space;(1)&space;}{&space;a&space;}_{&space;3&space;}^{&space;(1)&space;}" alt="\\ { z }_{ 1 }^{ (2) }={ \Theta }_{ 1,0 }^{ (1) }{ a }_{ 0 }^{ (1) }+{ \Theta }_{ 1,1 }^{ (1) }{ a }_{ 1 }^{ (1) }+{ \Theta }_{ 1,2 }^{ (1) }{ a }_{ 2 }^{ (1) }+{ \Theta }_{ 1,3 }^{ (1) }{ a }_{ 3 }^{ (1) }\\ { z }_{ 2 }^{ (2) }={ \Theta }_{ 2,0 }^{ (1) }{ a }_{ 0 }^{ (1) }+{ \Theta }_{ 2,1 }^{ (1) }{ a }_{ 1 }^{ (1) }+{ \Theta }_{ 2,2 }^{ (1) }{ a }_{ 2 }^{ (1) }+{ \Theta }_{ 2,3 }^{ (1) }{ a }_{ 3 }^{ (1) }" align="absmiddle" /></p>

<p><img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\\&space;{&space;a&space;}_{&space;1&space;}^{&space;(2)&space;}=g({&space;z&space;}_{&space;1&space;}^{&space;(2)&space;})\\&space;{&space;a&space;}_{&space;2&space;}^{&space;(2)&space;}=g({&space;z&space;}_{&space;2&space;}^{&space;(2)&space;})\\&space;{&space;z&space;}_{&space;1&space;}^{&space;(3)&space;}={&space;\Theta&space;}_{&space;1,0&space;}^{&space;(2)&space;}{&space;a&space;}_{&space;0&space;}^{&space;(2)&space;}+{&space;\Theta&space;}_{&space;1,1&space;}^{&space;(2)&space;}{&space;a&space;}_{&space;1&space;}^{&space;(2)&space;}+{&space;\theta&space;}_{&space;1,2&space;}^{&space;(2)&space;}{&space;a&space;}_{&space;2&space;}^{&space;(2)&space;}\\&space;{&space;z&space;}_{&space;2&space;}^{&space;(3)&space;}={&space;\Theta&space;}_{&space;2,0&space;}^{&space;(2)&space;}{&space;a&space;}_{&space;0&space;}^{&space;(2)&space;}+{&space;\Theta&space;}_{&space;2,1&space;}^{&space;(2)&space;}{&space;a&space;}_{&space;1&space;}^{&space;(2)&space;}+{&space;\Theta&space;}_{&space;2,2&space;}^{&space;(2)&space;}{&space;a&space;}_{&space;2&space;}^{&space;(2)&space;}\\&space;{&space;a&space;}_{&space;1&space;}^{&space;(3)&space;}=g({&space;z&space;}_{&space;1&space;}^{&space;(3)&space;})\\&space;{&space;a&space;}_{&space;2&space;}^{&space;(3)&space;}=g({&space;z&space;}_{&space;2&space;}^{&space;(3)&space;})\\&space;{&space;J&space;}_{&space;1&space;}=-{&space;y&space;}_{&space;1&space;}log({&space;a&space;}_{&space;1&space;}^{&space;(3)&space;})-(1-{&space;y&space;}_{&space;1&space;})log(1-{&space;a&space;}_{&space;1&space;}^{&space;(3)&space;})\\&space;{&space;J&space;}_{&space;2&space;}=-{&space;y&space;}_{&space;2&space;}log({&space;a&space;}_{&space;2&space;}^{&space;(3)&space;})-(1-{&space;y&space;}_{&space;2&space;})log(1-{&space;a&space;}_{&space;2&space;}^{&space;(3)&space;})" alt="\dpi{120} \\ { a }_{ 1 }^{ (2) }=g({ z }_{ 1 }^{ (2) })\\ { a }_{ 2 }^{ (2) }=g({ z }_{ 2 }^{ (2) })\\ { z }_{ 1 }^{ (3) }={ \Theta }_{ 1,0 }^{ (2) }{ a }_{ 0 }^{ (2) }+{ \Theta }_{ 1,1 }^{ (2) }{ a }_{ 1 }^{ (2) }+{ \theta }_{ 1,2 }^{ (2) }{ a }_{ 2 }^{ (2) }\\ { z }_{ 2 }^{ (3) }={ \Theta }_{ 2,0 }^{ (2) }{ a }_{ 0 }^{ (2) }+{ \Theta }_{ 2,1 }^{ (2) }{ a }_{ 1 }^{ (2) }+{ \Theta }_{ 2,2 }^{ (2) }{ a }_{ 2 }^{ (2) }\\ { a }_{ 1 }^{ (3) }=g({ z }_{ 1 }^{ (3) })\\ { a }_{ 2 }^{ (3) }=g({ z }_{ 2 }^{ (3) })\\ { J }_{ 1 }=-{ y }_{ 1 }log({ a }_{ 1 }^{ (3) })-(1-{ y }_{ 1 })log(1-{ a }_{ 1 }^{ (3) })\\ { J }_{ 2 }=-{ y }_{ 2 }log({ a }_{ 2 }^{ (3) })-(1-{ y }_{ 2 })log(1-{ a }_{ 2 }^{ (3) })" align="absmiddle" /></p>

<p>위와 같은 3-2-2층의 신경망이 있다. 각 node는 activation function으로 sigmoid 함수를 사용하므로 우리는 error를 정량화할 목적으로 classification에서 썼던 형태의 cost function을 사용할 수 있다.</p>

<p> </p>

<p><img src="https://latex.codecogs.com/gif.latex?J(\Theta&space;)=-\frac&space;{&space;1&space;}{&space;m&space;}&space;\sum&space;_{&space;i=1&space;}^{&space;m&space;}{&space;\sum&space;_{&space;k=1&space;}^{&space;K&space;}{&space;[{&space;y&space;}_{&space;k&space;}^{&space;(i)&space;}log({&space;({&space;h&space;}_{&space;\Theta&space;}({&space;x&space;}^{&space;(i)&space;}))&space;}_{&space;k&space;})+({&space;1-y&space;}_{&space;k&space;}^{&space;(i)&space;})log({&space;1-({&space;h&space;}_{&space;\Theta&space;}({&space;x&space;}^{&space;(i)&space;}))&space;}_{&space;k&space;})]&space;}&space;}&space;+" alt="J(\Theta )=-\frac { 1 }{ m } \sum _{ i=1 }^{ m }{ \sum _{ k=1 }^{ K }{ [{ y }_{ k }^{ (i) }log({ ({ h }_{ \Theta }({ x }^{ (i) })) }_{ k })+({ 1-y }_{ k }^{ (i) })log({ 1-({ h }_{ \Theta }({ x }^{ (i) })) }_{ k })] } } +" align="absmiddle" />  <img src="https://latex.codecogs.com/gif.latex?\frac&space;{&space;\lambda&space;}{&space;2m&space;}&space;\sum&space;_{&space;l=1&space;}^{&space;L-1&space;}{&space;\sum&space;_{&space;i=1&space;}^{&space;{&space;s&space;}_{&space;l&space;}&space;}{&space;\sum&space;_{&space;j=1&space;}^{&space;{&space;s&space;}_{&space;l+1&space;}&space;}{&space;{&space;({&space;\Theta&space;}_{&space;j,i&space;}^{&space;(l)&space;})&space;}^{&space;2&space;}&space;}&space;}&space;}" alt="\frac { \lambda }{ 2m } \sum _{ l=1 }^{ L-1 }{ \sum _{ i=1 }^{ { s }_{ l } }{ \sum _{ j=1 }^{ { s }_{ l+1 } }{ { ({ \Theta }_{ j,i }^{ (l) }) }^{ 2 } } } }" align="absmiddle" /></p>

<p>복잡해 보인다. 하지만 이 식에 너무 매몰되기보단 의미를 생각하면서 천천히 이해하면 된다.</p>

<p>단순히 m개의 훈련 데이터, K개의 클래스에 대하여 cost function J의 합을 계산할 뿐이다. 뒤에 있는 regularization term은 전체 신경망의 parameter를 집어넣었을 뿐이다.</p>

<p>각각의 parameter가 cost function에 얼마나 영향을 미치는지(얼마만큼의 error를 만들어 내는지) 알기 위해서는 일단 모든 <img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\Theta" alt="\dpi{120} \Theta" align="absmiddle" />에 대해 <img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\frac{\partial&space;J}{\partial&space;\Theta}" alt="\dpi{120} \frac{\partial J}{\partial \Theta}" align="absmiddle" />를 구해야한다. 이것을 구하면 gradient descent에서 했던 것처럼 적절한 learning rate를 곱하여 parameter를 update 할 수 있다. 이것이 역전파이다.</p>

<p> </p>

<p> </p>

<p> </p>

<h1 id="2-역전파를-위한-cost-function의-미분-계산">2. 역전파를 위한 cost function의 미분 계산</h1>

<hr />

<p>목표는<img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\frac{\partial&space;J}{\partial&space;\Theta}" alt="\dpi{120} \frac{\partial J}{\partial \Theta}" align="absmiddle" /> 를 계산하여 parameter를 update하는 것이다. 쉽게 계산하는 법은 다음과 같다.</p>

<p><img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\bigtriangleup^{(l)}&space;=&space;0\quad(l=1,2,...,L-1)" alt="\dpi{120} \bigtriangleup^{(l)} = 0\quad(l=1,2,...,L-1)" align="absmiddle" />      <img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\quad&space;\bigtriangleup^{(l)}" alt="\dpi{120} \quad \bigtriangleup^{(l)}" align="absmiddle" />은 <img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\Theta^{(l)}" alt="\dpi{120} \Theta^{(l)}" align="absmiddle" />과 같은 크기의 행렬이다. 아무튼 다음과 같은 행렬을 만들어놓고 다 0으로 초기화 시켜놓는다.</p>

<p><strong>for i = 1:m(m은 training dataset의 갯수),</strong></p>

<p><strong>   1. forward-propagation을 해서 output layer의 결과를 구한다. vector <img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;a^{&space;(L)&space;}" alt="\dpi{120} a^{ (L) }" align="absmiddle" /></strong></p>

<p><strong>   2. 계산결과값과 실제결과값의 차이 error vector를 다음과 같이 놓는다. <img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\delta^{(L)}&space;=&space;a^{(L)}-y" alt="\dpi{120} \delta^{(L)} = a^{(L)}-y" align="absmiddle" /></strong></p>

<p><strong>   3. 다음 공식에 따라 input layer를 제외한 층까지 역전파한다.  <img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;{&space;\delta&space;}^{&space;(l)&space;}=({&space;({&space;\Theta&space;}^{&space;(l)&space;})&space;}^{&space;T&space;}{&space;\delta&space;}^{&space;(l+1)&space;}).*{&space;a&space;}^{&space;(l)&space;}.*(1-{&space;a&space;}^{&space;(l)&space;})" alt="\dpi{120} { \delta }^{ (l) }=({ ({ \Theta }^{ (l) }) }^{ T }{ \delta }^{ (l+1) }).*{ a }^{ (l) }.*(1-{ a }^{ (l) })" align="absmiddle" /></strong></p>

<p><strong>   4. <img class="alignnone" src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\bigtriangleup^{(l)}=\bigtriangleup^{(l)}+\delta^{(l+1)}(a^{(l)})^{T}" alt="\dpi{120} \bigtriangleup^{(l)}=\bigtriangleup^{(l)}+\delta^{(l+1)}(a^{(l)})^{T}" align="absmiddle" />  ←!!주의 : <img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\delta^{(l+1)}" alt="\dpi{120} \delta^{(l+1)}" align="absmiddle" />을 대입할 때 bias unit을 뺀 <img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\delta" alt="\dpi{120} \delta" align="absmiddle" />를 적용한다. 왜냐하면 forward propagation을 할 때에 고정된 값(1)인 bias unit에는 영향을 주지 않기 때문이다. 다시 한 번 말하자면 bias unit은 전파를 한 다음 추가하는 node이다. output layer에는 bias node를 애초에 추가하지 않았기 때문에 상관없다. 아무튼 만약 실수로 bias unit까지 넣는다면 행렬곱셈을 할 때에 dimension이 맞지 않아 컴퓨터가 계산 오류를 일으킬 것이다.</strong></p>

<p><strong>end</strong></p>

<p>이상으로 다음과 같은 멋진 결론이 나온다.</p>

<p><img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\frac{\partial&space;J}{\partial\Theta^{(l)}}=\frac{1}{m}\bigtriangleup&space;^{(l)}" alt="\dpi{120} \frac{\partial J}{\partial\Theta^{(l)}}=\frac{1}{m}\bigtriangleup ^{(l)}" align="absmiddle" /></p>

<p> </p>

<p>위의 수식의 증명은 길고 재미없고 심지어 더럽기까지한 미분의 연속이다. 알 필요 없다. 단지 실제 데이터와 우리가 순전파 시켜 나온 예상값과의 에러를 이전 층으로 다시 전파시키는 짓을 반복해가며 cost-function J를 줄인다는 것만 알고 가도 충분하다.</p>

<p>처음의 그림에 나와있는 3-2-2 신경망에 대해서 한 번 연습해보자.</p>

<p><img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\\&space;\Theta^{(1)}=2\times&space;4&space;matrix\\&space;\Theta^{(2)}=2\times&space;3&space;matrix" alt="\dpi{120} \\ \Theta^{(1)}=2\times 4 matrix\\ \Theta^{(2)}=2\times 3 matrix" align="absmiddle" />     따라서,    <img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\bigtriangleup^{(1)}&space;=&space;\begin{bmatrix}&space;0&space;&amp;0&space;&amp;&space;0&space;&amp;&space;0\\&space;0&space;&amp;&space;0&space;&amp;&space;0&amp;&space;0&space;\end{bmatrix},\quad&space;\bigtriangleup^{(2)}&space;=&space;\begin{bmatrix}&space;0&space;&amp;0&space;&amp;&space;0&space;\\&space;0&space;&amp;&space;0&amp;&space;0&space;\end{bmatrix}" alt="\dpi{120} \bigtriangleup^{(1)} = \begin{bmatrix} 0 &amp;0 &amp; 0 &amp; 0\\ 0 &amp; 0 &amp; 0&amp; 0 \end{bmatrix},\quad \bigtriangleup^{(2)} = \begin{bmatrix} 0 &amp;0 &amp; 0 \\ 0 &amp; 0&amp; 0 \end{bmatrix}" align="absmiddle" /></p>

<p>for i=1:m</p>

<ul>
  <li>forward propagation, output layer error 계산</li>
</ul>

<p><img src="https://latex.codecogs.com/gif.latex?{&space;\delta&space;}_{&space;1&space;}^{&space;(3)&space;}={&space;a&space;}_{&space;1&space;}^{&space;(3)&space;}-{&space;y&space;}_{&space;1&space;}" alt="{ \delta }_{ 1 }^{ (3) }={ a }_{ 1 }^{ (3) }-{ y }_{ 1 }" align="absmiddle" /></p>

<p><img src="https://latex.codecogs.com/gif.latex?{&space;\delta&space;}_{&space;2&space;}^{&space;(3)&space;}={&space;a&space;}_{&space;2&space;}^{&space;(3)&space;}-{&space;y&space;}_{&space;2&space;}" alt="{ \delta }_{ 2 }^{ (3) }={ a }_{ 2 }^{ (3) }-{ y }_{ 2 }" align="absmiddle" /></p>

<p> </p>

<ul>
  <li>back propagation, hidden layer error 계산</li>
</ul>

<p><img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\\error\quad&space;back-propagation&gt;&gt;&space;\\&space;\\&space;\begin{bmatrix}&space;\delta_{1}^{(2)}&space;\\&space;\delta_{2}^{(2)}\end{bmatrix}=&space;\begin{bmatrix}&space;\Theta_{1,1}^{(2)}&space;&amp;&space;\Theta_{2,1}^{(2)}&space;\\&space;\Theta_{1,2}^{(2)}&space;&amp;&space;\Theta_{2,2}^{(2)}&space;\end{bmatrix}&space;\begin{bmatrix}&space;\delta_{1}^{(3)}&space;\\&space;\delta_{2}^{(3)}&space;\end{bmatrix}.*&space;\begin{bmatrix}&space;a_{1}^{(2)}&space;\\&space;a_{2}^{(2)}\end{bmatrix}.*&space;\begin{bmatrix}&space;1-a_{1}^{(2)}&space;\\&space;1-a_{2}^{(2)}\end{bmatrix}" alt="\dpi{120} \\error\quad back-propagation&gt;&gt; \\ \\ \begin{bmatrix} \delta_{1}^{(2)} \\ \delta_{2}^{(2)}\end{bmatrix}= \begin{bmatrix} \Theta_{1,1}^{(2)} &amp; \Theta_{2,1}^{(2)} \\ \Theta_{1,2}^{(2)} &amp; \Theta_{2,2}^{(2)} \end{bmatrix} \begin{bmatrix} \delta_{1}^{(3)} \\ \delta_{2}^{(3)} \end{bmatrix}.* \begin{bmatrix} a_{1}^{(2)} \\ a_{2}^{(2)}\end{bmatrix}.* \begin{bmatrix} 1-a_{1}^{(2)} \\ 1-a_{2}^{(2)}\end{bmatrix}" align="absmiddle" /></p>

<ul>
  <li>error 쌓아주기</li>
</ul>

<p><img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\\&space;\bigtriangleup&space;^{(1)}=\bigtriangleup&space;^{(1)}+\delta^{(2)}(a^{(1)})^{T}\\&space;\bigtriangleup&space;^{(2)}=\bigtriangleup&space;^{(2)}+\delta^{(3)}(a^{(2)})^{T}" alt="\dpi{120} \\ \bigtriangleup ^{(1)}=\bigtriangleup ^{(1)}+\delta^{(2)}(a^{(1)})^{T}\\ \bigtriangleup ^{(2)}=\bigtriangleup ^{(2)}+\delta^{(3)}(a^{(2)})^{T}" align="absmiddle" />        <img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\delta^{(2)}" alt="\dpi{120} \delta^{(2)}" align="absmiddle" />에서 bias node 빼주기.</p>

<p>end</p>

<ul>
  <li>쌓인 error 평균내서 cost function의 gradient 구하기</li>
</ul>

<p><img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\\&space;\frac{\partial&space;J}{\partial\Theta^{(1)}}=\frac{1}{m}\bigtriangleup&space;^{(1)}\\&space;\frac{\partial&space;J}{\partial\Theta^{(2)}}=\frac{1}{m}\bigtriangleup&space;^{(2)}" alt="\dpi{120} \\ \frac{\partial J}{\partial\Theta^{(1)}}=\frac{1}{m}\bigtriangleup ^{(1)}\\ \frac{\partial J}{\partial\Theta^{(2)}}=\frac{1}{m}\bigtriangleup ^{(2)}" align="absmiddle" /></p>

<p> </p>

<p>이제 gradient를 구했으니 적절한 learning rate를 이용해서 cost function J가 수렴할 때 까지 <img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\Theta" alt="\dpi{120} \Theta" align="absmiddle" />를 update하거나 python, matlab, octave 등의 라이브러리가 지원하는 optimization 함수를 사용하면 된다.</p>

<p>한 가지 덧붙이자면 input layer는 parameter Θ와 상관없이 항상 고정된 값(feature)를 가지므로 error가 없다. 따라서 지금 예시로 다루고 있는 신경망에서는 <img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\delta^{&space;(2)&space;},&space;\delta^{&space;(3)&space;}" alt="\dpi{120} \delta^{ (2) }, \delta^{ (3) }" align="absmiddle" /> 두 층의 에러만 구하면 된다.</p>

<p>지금까지 계산한 것을 그림으로 간단히 나타내면 다음과 같다.</p>

<p><img class="aligncenter size-medium wp-image-436" src="/images/2018/08/no-name-45-295x300.png" alt="" width="295" height="300" srcset="/images/2018/08/no-name-45-295x300.png 295w, /images/2018/08/no-name-45.png 548w" sizes="(max-width: 295px) 100vw, 295px" /></p>

<p> </p>

<p> </p>

  </article>
</div>

<div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/2020/03/09/%ED%95%A8%EC%88%98/">
            (클린코드) Chapter03 - 함수
            <small>09 Mar 2020</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2020/03/08/%EC%9D%98%EB%AF%B8-%EC%9E%88%EB%8A%94-%EC%9D%B4%EB%A6%84/">
            (클린코드) Chapter02 - 의미 있는 이름
            <small>08 Mar 2020</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2020/03/07/%EA%B9%A8%EB%81%97%ED%95%9C%EC%BD%94%EB%93%9C/">
            (클린코드) Chapter01 - 깨끗한 코드
            <small>07 Mar 2020</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>


<div class="comments">
  <h2>Comments</h2>
  <div id="disqus_thread"></div>
  <script>
var disqus_config = function () {
  this.page.url = 'http://localhost:4000/2018/08/10/12-back-propagation/'; // Replace PAGE_URL with your page's canonical URL variable
  this.page.identifier = '/2018/08/10/12-back-propagation'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
(function() {
  var d = document, s = d.createElement('script');
  s.src = '//wkimdev-gitblog.disqus.com/embed.js';
  s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
})();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
</div>


      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script>
      (function(document) {
        var toggle = document.querySelector('.sidebar-toggle');
        var sidebar = document.querySelector('#sidebar');
        var checkbox = document.querySelector('#sidebar-checkbox');

        document.addEventListener('click', function(e) {
          var target = e.target;

          if (target === toggle) {
            checkbox.checked = !checkbox.checked;
            e.preventDefault();
          } else if (checkbox.checked && !sidebar.contains(target)) {
            /* click outside the sidebar when sidebar is open */
            checkbox.checked = false;
          }
        }, false);
      })(document);
    </script>
    
    <script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-00000000-1', 'auto');
ga('send', 'pageview');
    </script>
    
  </body>
  
  <script id="dsq-count-scr" src="//wkimdev-gitblog.disqus.com/count.js" async></script>
  
</html>
