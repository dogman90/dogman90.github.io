<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      (머신러닝) 2. Linear Regression &middot; 공부 기록 블로그
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/main.css">
  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-precomposed.png">
  <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  
  <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_SVG"> </script>
  <script type="text/x-mathjax-config">
MathJax.Hub.Config({ tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], processEscapes: true } });
  </script>
  
</head>


  <body>

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <div class="sidebar-personal-info">
      <div class="sidebar-personal-info-section">
        <!-- <a href="http://gravatar.com/"> -->
          <a href="http://gravatar.com/"></a>
          <!-- <img src="http://www.gravatar.com/avatar/?s=350" title="View on Gravatar" alt="View on Gravatar" /> -->
          <!-- <img src="https://s.gravatar.com/avatar/fedd0cb9bf32323502b502a383136608?s=80" title="View on Gravatar" alt="View on Gravatar" style="margin:auto;" /> -->
          <img src="https://s.gravatar.com/avatar/fedd0cb9bf32323502b502a383136608?s=80" title="View on Gravatar" alt="View on Gravatar" style="margin:auto;" />
          

          
        </a>
      </div>
      <div class="sidebar-personal-info-section" >
        <p>개발자가 되고픈 사람. <br> 제대로 노력해서 1보를 이루자</p>
      </div>
      
    </div>
  </div>

  <nav class="sidebar-nav">
    
      
      
      

      

      <span class="">
        <a class="sidebar-nav-item " href="/">
          Home
        </a>

        
      </span>

    
      
      
      

      

      <span class="foldable">
        <a class="sidebar-nav-item " href="/blog/">
          Blog
        </a>

        
          
            
            
            
              <a class="sidebar-nav-item sidebar-nav-item-sub " href="/blog/categories/">
                Categories
              </a>
          
        
          
            
            
            
              <a class="sidebar-nav-item sidebar-nav-item-sub " href="/blog/tags/">
                Tags
              </a>
          
        
      </span>

    
      
      
      

      

      <span class="">
        <a class="sidebar-nav-item " href="/about/">
          About
        </a>

        
      </span>

    

    <span class="sidebar-nav-item">Currently v1.0.0</span>
  </nav>

  <div class="sidebar-item">
    <p>
    &copy; 2020 nobbaggu. This work is liscensed under <a href="http://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0</a>.
    </p>
  </div>

  <div class="sidebar-item">
    <p>
    Powered by <a href="http://jekyllrb.com">jekyll</a>
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <!-- <a href="/" title="Home" title="공부 기록 블로그">
              <img class="masthead-logo" src=""/>
            </a> -->
            <small>개발자가 되고 싶은 사람</small>
          </h3>
        </div>
      </div>

      <div class="container content">
        <div class="post">
  <h1 class="post-title">(머신러닝) 2. Linear Regression</h1>
  <span class="post-date">08 Aug 2018</span>
   | 
  
    <a href="/blog/tags/#ai" class="post-tag">AI</a>
  
    <a href="/blog/tags/#cost-function" class="post-tag">cost function</a>
  
    <a href="/blog/tags/#gradient-descent" class="post-tag">gradient descent</a>
  
    <a href="/blog/tags/#hypothesis-function" class="post-tag">hypothesis function</a>
  
    <a href="/blog/tags/#linear-regression" class="post-tag">linear regression</a>
  
    <a href="/blog/tags/#machine-learning" class="post-tag">machine learning</a>
  
    <a href="/blog/tags/#가설함수" class="post-tag">가설함수</a>
  
    <a href="/blog/tags/#경사하강법" class="post-tag">경사하강법</a>
  
    <a href="/blog/tags/#머신러닝" class="post-tag">머신러닝</a>
  
    <a href="/blog/tags/#비용함수" class="post-tag">비용함수</a>
  
    <a href="/blog/tags/#선형회귀" class="post-tag">선형회귀</a>
  
    <a href="/blog/tags/#인공지능" class="post-tag">인공지능</a>
  
  
  <article>
    <p>Linear Regression (선형회귀)은 데이터 패턴과 비슷한 선형함수를 구하는 일이다. 물론 모든 데이터가 linear하지는 않다. 이런 경우에는 몇 가지 테크닉을 사용해 linear regression 으로 문제를 풀 수 있다.</p>

<p>Linear Regression with One Variable(단변수 선형)회귀는 <strong>1개의 input(x)이 들어왔을 때 1개의 output(y)을 예측하는 모델을 구하는 문제이다</strong>. Linear Regression은 지도학습(supervised learning)에 해당하는 문제이다.</p>

<p> </p>

<h1 id="1-hypothesis-function"><span style="font-size: 18pt;"><strong>1. Hypothesis Function</strong></span></h1>

<hr />

<p><strong>Hypothesis Function이란 주어진 데이터들의 관계식을 나타내는 함수이다.</strong> input이 한 개 인 경우의 Linear Regression 의 hypothesis function의 형태는 다음과 같다.</p>

<p><img src="https://latex.codecogs.com/gif.latex?\hat&space;{&space;y&space;}&space;=&space;{&space;h&space;}_{&space;\theta&space;}(x)&space;=&space;{&space;\theta&space;}_{&space;0&space;}+{&space;\theta&space;}_{&space;1&space;}x" alt="\hat { y } = { h }_{ \theta }(x) = { \theta }_{ 0 }+{ \theta }_{ 1 }x" align="absmiddle" /></p>

<p>위는 고등학교 때 배운 1차함수식이다. model이 주어진 훈련 dataset을 얼마나 작은 오차로 잘 표현하는지는<img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\theta_{0}" alt="\dpi{120} \theta_{0}" align="absmiddle" />(offset) 와<img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\theta_{1}" alt="\dpi{120} \theta_{1}" align="absmiddle" /> (기울기)의 값에 달려있다.<img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\theta_{0}" alt="\dpi{120} \theta_{0}" align="absmiddle" /> <strong>와 <img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\theta_{1}" alt="\dpi{120} \theta_{1}" align="absmiddle" />은 모델을 결정하는 인자로서 parameter라고 한다</strong>.</p>

<p><img class="aligncenter wp-image-232" src="/images/2018/08/1.png" alt="" width="517" height="239" srcset="/images/2018/08/1.png 777w, /images/2018/08/1-300x139.png 300w, /images/2018/08/1-768x355.png 768w" sizes="(max-width: 517px) 100vw, 517px" /></p>

<p> </p>

<p><strong>regression은 결과를 피드백 해주어 함수의 parameter값을 최적화 하는 방법이다. 결국 <img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\theta_{0}" alt="\dpi{120} \theta_{0}" align="absmiddle" />와<img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\theta_{1}" alt="\dpi{120} \theta_{1}" align="absmiddle" />  parameter들의 best 값을 찾아가는 일이다</strong>. 위 그림에서 데이터를 가장 잘 따르는 parameter는<img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\theta_{0}=1" alt="\dpi{120} \theta_{0}=1" align="absmiddle" /> ,<img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\theta_{1}=\frac{1}{2}" alt="\dpi{120} \theta_{1}=\frac{1}{2}" align="absmiddle" />  이다. 어떤 parameter가 좋은 모델을 만드는지 평가할 수 있는 지표가 필요한데 그것이 바로 cost function이다.</p>

<p> </p>

<p> </p>

<h1 id="2-cost-function"><span style="font-size: 18pt;"><strong>2. Cost Function</strong></span></h1>

<hr />

<p>모델과 훈련 dataset 사이의 오차를 나타내주는 함수가 바로 Cost Function이다. Cost Function은 수식적으로 어떻게 표현되는지 보자.</p>

<p><img src="https://latex.codecogs.com/gif.latex?J({&space;\theta&space;}_{&space;0&space;},{&space;\theta&space;}_{&space;1&space;})\quad&space;=\quad&space;\frac&space;{&space;1&space;}{&space;2m&space;}&space;\sum&space;_{&space;i=1&space;}^{&space;m&space;}{&space;{&space;({&space;\hat&space;{&space;y&space;}&space;}_{&space;i&space;}-{&space;y&space;}_{&space;i&space;})&space;}^{&space;2&space;}\quad&space;}&space;=\quad&space;\frac&space;{&space;1&space;}{&space;2m&space;}&space;\sum&space;_{&space;i=1&space;}^{&space;m&space;}{&space;{&space;({&space;h&space;}_{&space;\theta&space;}({&space;x&space;}_{&space;i&space;})-{&space;y&space;}_{&space;i&space;})&space;}^{&space;2&space;}&space;}&space;\quad&space;=\quad&space;\frac&space;{&space;1&space;}{&space;2m&space;}&space;\sum&space;_{&space;i=1&space;}^{&space;m&space;}{&space;{&space;({&space;\theta&space;}_{&space;0&space;}+{&space;\theta&space;}_{&space;1&space;}{&space;x&space;}_{&space;i&space;}-{&space;y&space;}_{&space;i&space;})&space;}^{&space;2&space;}&space;}" alt="J({ \theta }_{ 0 },{ \theta }_{ 1 })\quad =\quad \frac { 1 }{ 2m } \sum _{ i=1 }^{ m }{ { ({ \hat { y } }_{ i }-{ y }_{ i }) }^{ 2 }\quad } =\quad \frac { 1 }{ 2m } \sum _{ i=1 }^{ m }{ { ({ h }_{ \theta }({ x }_{ i })-{ y }_{ i }) }^{ 2 } } \quad =\quad \frac { 1 }{ 2m } \sum _{ i=1 }^{ m }{ { ({ \theta }_{ 0 }+{ \theta }_{ 1 }{ x }_{ i }-{ y }_{ i }) }^{ 2 } }" align="absmiddle" /></p>

<p>여기서 m은 훈련시킨 데이터의 총 갯수(size of the training data set)이다. <strong>간단히 말해서 error = 예측값 – 데이터의 결과값 이라고 했을때 모든 training data sample에 대해서 error의 제곱을 더한 값 <img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\sum&space;error^{2}" alt="\dpi{120} \sum error^{2}" align="absmiddle" /> 을 총 훈련 데이터의 갯수 m으로 나눠서 평균값을 구한것이(사실 m이 아니라 2m으로 나누지만) cost function이다.</strong> <strong>cost function의 크기가 작을수록 실제와의 오차가 작은 모델이다.</strong> m이 아닌 1/2m로 나누는 이유는 나중에 미분을 하였을 때 튀어나오는 2를 없애고 수식을 조금이나마 더 간단히 하기위함이다.</p>

<p><img class=" wp-image-233" src="/images/2018/08/2.jpg" alt="" width="477" height="265" srcset="/images/2018/08/2.jpg 625w, /images/2018/08/2-300x167.jpg 300w" sizes="(max-width: 477px) 100vw, 477px" /></p>

<p>위 그림에는 5개의 데이터 set이 있다. 예측값과 실제 결과값 사이의 에러를 e1, e2, e3, e4, e5라 했을 때 각각의 차이의 제곱을 모두 더한 값이 cost값이다. 이 cost값은<img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\theta_{0}" alt="\dpi{120} \theta_{0}" align="absmiddle" /> 와<img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\theta_{1}" alt="\dpi{120} \theta_{1}" align="absmiddle" /> 에 의해 변한다. 아까 말한 linear regression 방법을 써 최적의 <img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\theta_{0}" alt="\dpi{120} \theta_{0}" align="absmiddle" /> 와<img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\theta_{1}" alt="\dpi{120} \theta_{1}" align="absmiddle" /> 을 찾을 수 있다.</p>

<p> </p>

<p> </p>

<h1 id="3-gradient-descent"><span style="font-size: 18pt;"><strong>3. Gradient Descent</strong></span></h1>

<hr />

<p>Gradient Descent라는 것은 어떠한 함수가 최소값을 가지게하는 point를 찾아가는 일종의 수학 테크닉이다.이 gradient descent를 통해서 cost function이 최소가 되게하는<img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;(\theta_{0},\theta_{1})" alt="\dpi{120} (\theta_{0},\theta_{1})" align="absmiddle" />을 구할 수 있다.</p>

<p>graident descent의 기본적인 idea는 어떠한 함수의 gradient가 가지는 성질중 한 가지로부터 기인한다. cost function J는<img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\theta_{0}" alt="\dpi{120} \theta_{0}" align="absmiddle" /> 와<img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\theta_{1}" alt="\dpi{120} \theta_{1}" align="absmiddle" /> 의 함수이다. 이 함수의 gradient는 다음과 같다.</p>

<p><img src="https://latex.codecogs.com/gif.latex?\triangledown&space;J({&space;\theta&space;}_{&space;0&space;},{&space;\theta&space;}_{&space;1&space;})\quad&space;=\quad&space;(\frac&space;{&space;\partial&space;J&space;}{&space;\partial&space;{&space;\theta&space;}_{&space;0&space;}&space;}&space;,\frac&space;{&space;\partial&space;J&space;}{&space;\partial&space;{&space;\theta&space;}_{&space;1&space;}&space;}&space;)" alt="\triangledown J({ \theta }_{ 0 },{ \theta }_{ 1 })\quad =\quad (\frac { \partial J }{ \partial { \theta }_{ 0 } } ,\frac { \partial J }{ \partial { \theta }_{ 1 } } )" align="absmiddle" /></p>

<p>어떠한 함수의 gradient는하나의 vector이다. <strong>gradient 벡터의 특징은 그 좌표에서 함수가 가장 빠르게 증가하는 방향을 가르킨다.</strong></p>

<p>cost function J의 gradient를 이용하여 J가 줄어드는 방향으로<img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;(\theta_{0},\theta_{1})" alt="\dpi{120} (\theta_{0},\theta_{1})" align="absmiddle" /> 을 움직여 새로운<img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;(\theta_{0},\theta_{1})" alt="\dpi{120} (\theta_{0},\theta_{1})" align="absmiddle" /> 을 얻으려 한다. 이 때 새로운<img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;(\theta_{0},\theta_{1})" alt="\dpi{120} (\theta_{0},\theta_{1})" align="absmiddle" /> 은 다음과 같이 결정된다.</p>

<p><img src="https://latex.codecogs.com/gif.latex?({&space;\theta&space;}_{&space;0&space;},{&space;\theta&space;}_{&space;1&space;})&space;:=&space;({&space;\theta&space;}_{&space;0&space;},{&space;\theta&space;}_{&space;1&space;})-&space;\alpha&space;\triangledown&space;J({&space;\theta&space;}_{&space;0&space;},{&space;\theta&space;}_{&space;1&space;})&space;=&space;({&space;\theta&space;}_{&space;0&space;}-\alpha&space;\frac&space;{&space;\partial&space;J&space;}{&space;\partial&space;{&space;\theta&space;}_{&space;0&space;}&space;}&space;,&space;{&space;\theta&space;}_{&space;1&space;}-\alpha&space;\frac&space;{&space;\partial&space;J&space;}{&space;\partial&space;{&space;\theta&space;}_{&space;1&space;}&space;}&space;)" alt="({ \theta }_{ 0 },{ \theta }_{ 1 }) := ({ \theta }_{ 0 },{ \theta }_{ 1 })- \alpha \triangledown J({ \theta }_{ 0 },{ \theta }_{ 1 }) = ({ \theta }_{ 0 }-\alpha \frac { \partial J }{ \partial { \theta }_{ 0 } } , { \theta }_{ 1 }-\alpha \frac { \partial J }{ \partial { \theta }_{ 1 } } )" align="absmiddle" /></p>

<p>이 식에 의해 계산된 새로운<img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;(\theta_{0},\theta_{1})" alt="\dpi{120} (\theta_{0},\theta_{1})" align="absmiddle" /> 는 gradient 벡터의 성질때문에 cost function이 작아지는 방향으로 이동한다. 이해를 돕기위해 다음 그림을 보자.</p>

<p><img class="aligncenter wp-image-234" src="/images/2018/08/3.png" alt="" width="548" height="216" srcset="/images/2018/08/3.png 900w, /images/2018/08/3-300x118.png 300w, /images/2018/08/3-768x303.png 768w" sizes="(max-width: 548px) 100vw, 548px" /></p>

<p>적당한 alpha라는 factor를 곱하여 기울기의 반대방향으로 이동시키면 점의 위치는 local minimum 방향으로 이동하게 된다. 어떤<img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;(\theta_{0},\theta_{1})" alt="\dpi{120} (\theta_{0},\theta_{1})" align="absmiddle" /> 을 잡아도 위의 공식에 따르면 local minimum의 방향으로 움직이게 된다. <strong>이 작업을 cost function의 값이 수렴할 때 까지 충분한 횟수로 반복해주면 되는것이다.</strong></p>

<p>이 때 alpha가 너무 작으면 반복횟수가 커져야하고 학습속도가 느려진다. alpha가 너무 크면 진동하는 경향이 커져 수렴하지 못할 수가 있다. <strong>alpha는</strong> <strong>학습속도를 결정하므로 learning rate라고 부른다.</strong></p>

<p>다시 위에서 나온 식을 마저 정리해보자. J의<img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\theta_{0}" alt="\dpi{120} \theta_{0}" align="absmiddle" /> 와<img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\theta_{1}" alt="\dpi{120} \theta_{1}" align="absmiddle" /> 에 대한 편미분은 위의 hypothesis function과 cost function의 식을 적용해서 구하면 다음과 같이 정리된다.</p>

<p> </p>

<p><img src="https://latex.codecogs.com/gif.latex?\frac&space;{&space;\partial&space;J&space;}{&space;\partial&space;{&space;\theta&space;}_{&space;0&space;}&space;}&space;\quad&space;=\quad&space;\frac&space;{&space;1&space;}{&space;m&space;}&space;\sum&space;_{&space;i=1&space;}^{&space;m&space;}{&space;{&space;({&space;\theta&space;}_{&space;0&space;}+{&space;\theta&space;}_{&space;1&space;}{&space;x&space;}_{&space;i&space;}-{&space;y&space;}_{&space;i&space;})&space;}&space;}&space;\quad&space;=\quad&space;\frac&space;{&space;1&space;}{&space;m&space;}&space;\sum&space;_{&space;i=1&space;}^{&space;m&space;}{&space;{&space;({&space;h&space;}_{&space;\theta&space;}({&space;x&space;}_{&space;i&space;})-{&space;y&space;}_{&space;i&space;})&space;}&space;}" alt="\frac { \partial J }{ \partial { \theta }_{ 0 } } \quad =\quad \frac { 1 }{ m } \sum _{ i=1 }^{ m }{ { ({ \theta }_{ 0 }+{ \theta }_{ 1 }{ x }_{ i }-{ y }_{ i }) } } \quad =\quad \frac { 1 }{ m } \sum _{ i=1 }^{ m }{ { ({ h }_{ \theta }({ x }_{ i })-{ y }_{ i }) } }" align="absmiddle" /></p>

<p><img src="https://latex.codecogs.com/gif.latex?\frac&space;{&space;\partial&space;J&space;}{&space;\partial&space;{&space;\theta&space;}_{&space;1&space;}&space;}&space;\quad&space;=\quad&space;\frac&space;{&space;1&space;}{&space;m&space;}&space;\sum&space;_{&space;i=1&space;}^{&space;m&space;}{&space;{&space;({&space;\theta&space;}_{&space;0&space;}+{&space;\theta&space;}_{&space;1&space;}{&space;x&space;}_{&space;i&space;}-{&space;y&space;}_{&space;i&space;})\cdot&space;}{&space;x&space;}_{&space;i&space;}&space;}&space;\quad&space;=\quad&space;\frac&space;{&space;1&space;}{&space;m&space;}&space;\sum&space;_{&space;i=1&space;}^{&space;m&space;}{&space;{&space;({&space;h&space;}_{&space;\theta&space;}({&space;x&space;}_{&space;i&space;})-{&space;y&space;}_{&space;i&space;})\cdot&space;}&space;}&space;{&space;x&space;}_{&space;i&space;}" alt="\frac { \partial J }{ \partial { \theta }_{ 1 } } \quad =\quad \frac { 1 }{ m } \sum _{ i=1 }^{ m }{ { ({ \theta }_{ 0 }+{ \theta }_{ 1 }{ x }_{ i }-{ y }_{ i })\cdot }{ x }_{ i } } \quad =\quad \frac { 1 }{ m } \sum _{ i=1 }^{ m }{ { ({ h }_{ \theta }({ x }_{ i })-{ y }_{ i })\cdot } } { x }_{ i }" align="absmiddle" /></p>

<p>이 식을 적용하면 다음과 같은 최종식이 나온다.</p>

<p><img src="https://latex.codecogs.com/gif.latex?\\({&space;\theta&space;}_{&space;0&space;},{&space;\theta&space;}_{&space;1&space;})&space;:=&space;({&space;\theta&space;}_{&space;0&space;}-\alpha&space;\frac&space;{&space;\partial&space;J&space;}{&space;\partial&space;{&space;\theta&space;}_{&space;0&space;}&space;}&space;,&space;{&space;\theta&space;}_{&space;1&space;}-\alpha&space;\frac&space;{&space;\partial&space;J&space;}{&space;\partial&space;{&space;\theta&space;}_{&space;1&space;}&space;}&space;)&space;\\=&space;\&space;({&space;\theta&space;}_{&space;0&space;}-\frac&space;{&space;\alpha&space;}{&space;m&space;}&space;\sum&space;_{&space;i=1&space;}^{&space;m&space;}{&space;{&space;({&space;h&space;}_{&space;\theta&space;}({&space;x&space;}_{&space;i&space;})-{&space;y&space;}_{&space;i&space;})&space;}&space;}&space;,{&space;\theta&space;}_{&space;1&space;}-\frac&space;{&space;\alpha&space;}{&space;m&space;}&space;\sum&space;_{&space;i=1&space;}^{&space;m&space;}{&space;{&space;(({&space;h&space;}_{&space;\theta&space;}({&space;x&space;}_{&space;i&space;})-{&space;y&space;}_{&space;i&space;})&space;}&space;}&space;{&space;\cdot&space;x&space;}_{&space;i&space;})" alt="\\({ \theta }_{ 0 },{ \theta }_{ 1 }) := ({ \theta }_{ 0 }-\alpha \frac { \partial J }{ \partial { \theta }_{ 0 } } , { \theta }_{ 1 }-\alpha \frac { \partial J }{ \partial { \theta }_{ 1 } } ) \\= \ ({ \theta }_{ 0 }-\frac { \alpha }{ m } \sum _{ i=1 }^{ m }{ { ({ h }_{ \theta }({ x }_{ i })-{ y }_{ i }) } } ,{ \theta }_{ 1 }-\frac { \alpha }{ m } \sum _{ i=1 }^{ m }{ { (({ h }_{ \theta }({ x }_{ i })-{ y }_{ i }) } } { \cdot x }_{ i })" align="absmiddle" /></p>

<p>이 최종식을 적용하여<img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\theta_{0}" alt="\dpi{120} \theta_{0}" align="absmiddle" /> 와<img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\theta_{1}" alt="\dpi{120} \theta_{1}" align="absmiddle" /> 이 수렴할 때 까지 update를 해주면 데이터들에 가장 fit하는 모델의<img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\theta_{0}" alt="\dpi{120} \theta_{0}" align="absmiddle" /> 와<img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\theta_{1}" alt="\dpi{120} \theta_{1}" align="absmiddle" /> 이 나온다. alpha값이 고정되어있어도 local minimum에 접근할수록 기울기가 작아진다. 따라서 자동적으로 더 작은 step씩 이동하게되고 수렴할 수 있게 되는것이다.</p>

<p> </p>

<p> </p>

<h1 id="4-summary"><strong>4. Summary</strong></h1>

<hr />

<p>Linear Regression withe One Variable의 내용을 정리해보자.</p>

<p><span style="font-size: 12pt;"><strong>1) 1차함수 형태의 hypothesis function을 세우고 무작위로 <img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\theta_{0}" alt="\dpi{120} \theta_{0}" align="absmiddle" />과 <img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\theta_{1}" alt="\dpi{120} \theta_{1}" align="absmiddle" />을 set한다.</strong></span></p>

<p><span style="font-size: 12pt;"><strong>2) learning rate alpha를 정하고 위의 update식에 따라 model parameter(<img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\theta_{0}" alt="\dpi{120} \theta_{0}" align="absmiddle" /> <img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\theta_{1}" alt="\dpi{120} \theta_{1}" align="absmiddle" />)을 반복 학습시킨다. 이를 gradient descent(경사하강법)이라고 한다.</strong></span></p>

<p><span style="font-size: 12pt;"><strong>3) <img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;J(\theta)" alt="\dpi{120} J(\theta)" align="absmiddle" />가 수렴했으면 지금의 <img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\theta_{0}" alt="\dpi{120} \theta_{0}" align="absmiddle" />와 <img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\theta_{1}" alt="\dpi{120} \theta_{1}" align="absmiddle" />을</strong> 이<strong> model parameter로 사용한다</strong></span></p>

  </article>
</div>

<div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/2020/03/09/%ED%95%A8%EC%88%98/">
            (클린코드) Chapter03 - 함수
            <small>09 Mar 2020</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2020/03/08/%EC%9D%98%EB%AF%B8-%EC%9E%88%EB%8A%94-%EC%9D%B4%EB%A6%84/">
            (클린코드) Chapter02 - 의미 있는 이름
            <small>08 Mar 2020</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2020/03/07/%EA%B9%A8%EB%81%97%ED%95%9C%EC%BD%94%EB%93%9C/">
            (클린코드) Chapter01 - 깨끗한 코드
            <small>07 Mar 2020</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>


<div class="comments">
  <h2>Comments</h2>
  <div id="disqus_thread"></div>
  <script>
var disqus_config = function () {
  this.page.url = 'http://localhost:4000/2018/08/08/2-linear-regression/'; // Replace PAGE_URL with your page's canonical URL variable
  this.page.identifier = '/2018/08/08/2-linear-regression'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
(function() {
  var d = document, s = d.createElement('script');
  s.src = '//wkimdev-gitblog.disqus.com/embed.js';
  s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
})();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
</div>


      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script>
      (function(document) {
        var toggle = document.querySelector('.sidebar-toggle');
        var sidebar = document.querySelector('#sidebar');
        var checkbox = document.querySelector('#sidebar-checkbox');

        document.addEventListener('click', function(e) {
          var target = e.target;

          if (target === toggle) {
            checkbox.checked = !checkbox.checked;
            e.preventDefault();
          } else if (checkbox.checked && !sidebar.contains(target)) {
            /* click outside the sidebar when sidebar is open */
            checkbox.checked = false;
          }
        }, false);
      })(document);
    </script>
    
    <script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-00000000-1', 'auto');
ga('send', 'pageview');
    </script>
    
  </body>
  
  <script id="dsq-count-scr" src="//wkimdev-gitblog.disqus.com/count.js" async></script>
  
</html>
